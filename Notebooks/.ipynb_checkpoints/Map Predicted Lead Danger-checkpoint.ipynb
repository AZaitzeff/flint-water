{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In this notebook we want to generate maps of the lead testing data.\n",
    "# Specifically, we will use the lead testing data as a training set\n",
    "# to make predictions about parcels that have not been tested yet.\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We will predict the untested parcel data given the testing data joined with the parcel data.\n",
    "parcel_df = pd.read_csv('./parcel_geo_data.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will train our predictions on the untested parcels from the residential testing data.\n",
    "#\n",
    "# It would be very interesting to do the same with the sentinel data and see how the analyses compare.\n",
    "\n",
    "target_df = pd.read_csv('./residential_test_data.csv', usecols=['Lead (ppb)', 'Copper (ppb)'])\n",
    "lead_test_df_ = pd.read_csv('./residential_test_data.csv', usecols=parcel_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are the columns that we have been using for the analysis.\n",
    "# See Flint Prediction Exploration notebook for more thorough explanation.\n",
    "\n",
    "columns = ['Property Zip Code','Owner Type',\n",
    "           'Owner State', 'Homestead', 'Homestead Percent', 'HomeSEV','Land Value', 'Land Improvements Value',\n",
    "           'Residential Building Value', 'Residential Building Style','Commercial Building Value',\n",
    "           'Building Storeys', 'Parcel Acres', 'Rental', 'Use Type', 'Prop Class',\n",
    "           'Old Prop class', 'Year Built', 'USPS Vacancy', 'Zoning', 'Future Landuse', 'DRAFT Zone',\n",
    "           'Housing Condition 2012', 'Housing Condition 2014', 'Commercial Condition 2013', 'Latitude',\n",
    "           'Longitude', 'Hydrant Type']\n",
    "\n",
    "dummy_cols = ['Property Zip Code', 'Owner Type', 'Residential Building Style', 'Homestead',\n",
    "              'Building Storeys', 'Rental', 'Use Type', 'Prop Class', 'Old Prop class', 'USPS Vacancy',\n",
    "              'Housing Condition 2012', 'Housing Condition 2014',\n",
    "              'Commercial Condition 2013', 'Hydrant Type']\n",
    "\n",
    "drop_cols = ['Zoning', 'Future Landuse', 'DRAFT Zone', 'Owner State', 'Latitude', 'Longitude', 'Year Built']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lead_test_df_.fillna('n/a', inplace=True)\n",
    "lead_test_df = pd.get_dummies(lead_test_df_[columns], columns=dummy_cols)\n",
    "lead_test_df['Year Category'] = pd.cut(lead_test_df['Year Built'], bins=[0,1927,1940,1950,1954,1959,2013])\n",
    "lead_test_df = pd.get_dummies(lead_test_df,columns=['Year Category'])\n",
    "lead_test_df.drop(drop_cols, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find parcels that test > 15 ppb in the training set.\n",
    "Ytrain = target_df['Lead (ppb)'] > 15\n",
    "Xtrain = lead_test_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now, we will predict the lead probability of a dangerous lead level in the parcels that\n",
    "# have not been tested.  The first step in doing this is to find which property ids show\n",
    "# up in the training set.\n",
    "\n",
    "tested_pid_df = pd.read_csv('residential_test_data.csv', usecols=['PID no Dash'])\n",
    "pids = list(set(tested_pid_df['PID no Dash']))\n",
    "\n",
    "# Iterate through the parcels and determine which have been tested.\n",
    "mask = []\n",
    "for pid in parcel_df['PID no Dash'].values:\n",
    "    mask.append(pid not in pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we construct a our test set in the same way that we created our training set.\n",
    "\n",
    "# Remove parcels that haven't been tested.\n",
    "test_df_ = parcel_df[mask].copy()\n",
    "\n",
    "# Proceed as before when we created the training set.\n",
    "test_df = pd.get_dummies(test_df_[columns], columns=dummy_cols)\n",
    "test_df['Year Category'] = pd.cut(test_df['Year Built'], bins=[0,1927,1940,1950,1954,1959,2013])\n",
    "test_df = pd.get_dummies(test_df,columns=['Year Category'])\n",
    "test_df.drop(drop_cols, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the numpy array from the dataframe.\n",
    "Xtest = test_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predict on the numpy array.\n",
    "import xgboost as xgb\n",
    "\n",
    "param_dict = {'colsample_bytree': 0.75, 'colsample_bylevel': 1, 'nthread': 1,\n",
    "              'n_estimators': 32, 'subsample': 1, 'max_depth': 4, 'gamma': 0}\n",
    "xg = xgb.XGBClassifier(colsample_bytree=param_dict['colsample_bytree'], \n",
    "                           colsample_bylevel=param_dict['colsample_bylevel'],\n",
    "                           n_estimators=param_dict['n_estimators'],\n",
    "                           subsample=param_dict['subsample'],\n",
    "                           max_depth=param_dict['max_depth'],\n",
    "                           gamma=param_dict['gamma'],\n",
    "                           seed=42)\n",
    "\n",
    "xg.fit(Xtrain, Ytrain)\n",
    "yhat = xg.predict_proba(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the latitude and longitude of each predicted parcel.\n",
    "location_data = test_df_[['Latitude', 'Longitude']].copy()\n",
    "# Create a new column with the probability of danger corresponding to the parcels.\n",
    "location_data['prob'] = yhat[:,1]\n",
    "\n",
    "# These rows do not have lat/lon data.  They could probably be found if someone took the time\n",
    "# to enter them in by hand.\n",
    "location_data.drop([39473, 39474, 39475, 41134, 41170, \n",
    "                    41213, 52344, 52345, 52346, 52347], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the most dangerous locations.  There are so many locations that plotting all of them\n",
    "# would just make a messy map.\n",
    "\n",
    "# You can adjust this threshold until you get the number of points that you think will\n",
    "# fit nicely on the map.  I usually shoot for 100-200.\n",
    "\n",
    "thold = 0.25\n",
    "\n",
    "lons = location_data['Longitude'][location_data['prob'] > thold].values\n",
    "lats = location_data['Latitude'][location_data['prob'] > thold].values\n",
    "probs = location_data['prob'][location_data['prob'] > thold].values\n",
    "\n",
    "# This is the number of points that will be plotted on the map.\n",
    "len(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(12,12)\n",
    "\n",
    "# Center the map on flint.\n",
    "m = Basemap(llcrnrlon=-83.77, llcrnrlat=42.97, urcrnrlon=-83.61, urcrnrlat=43.08,\n",
    "            projection='lcc', lat_1=42, lat_2=44, lon_0=-83.65,\n",
    "            resolution='i', area_thresh=100)\n",
    "\n",
    "m.drawcoastlines()\n",
    "m.drawstates()\n",
    "m.drawcountries()\n",
    "\n",
    "# Get the locations on the image of the dangerous homes.\n",
    "x, y = m(lons, lats)\n",
    "\n",
    "# Plot each point.  Larger and darker dots correspond to more danger.\n",
    "colors = probs/probs.max()\n",
    "for i, coords in enumerate(zip(x,y)):\n",
    "    m.plot(coords[0], coords[1], markersize=25*probs[i], marker='o', color=(colors[i], 0.0, 1 - colors[i]), alpha=0.5)\n",
    "\n",
    "# This code makes something like a heat map.  It is just something I experimented with, but it could be\n",
    "# improved to something better.  Feel free to uncomment the code and see what it does.\n",
    "\n",
    "'''\n",
    "db = 0 # bin padding\n",
    "lon_bins = np.linspace(min(lons.astype(float))-db, \n",
    "                       max(lons.astype(float))+db, 25+1) # 10 bins\n",
    "lat_bins = np.linspace(min(lats.astype(float))-db, \n",
    "                       max(lats.astype(float))+db, 25+1) # 13 bins\n",
    "\n",
    "density, _, _ = np.histogram2d(lats.astype(float), lons.astype(float), [lat_bins, lon_bins])\n",
    "lon_bins_2d, lat_bins_2d = np.meshgrid(lon_bins, lat_bins)\n",
    "xs, ys = m(lon_bins_2d, lat_bins_2d)\n",
    "\n",
    "plt.pcolormesh(xs, ys, density, cmap=\"jet\", alpha=0.2)\n",
    "'''\n",
    "\n",
    "# If someone can get matplotlib to read .pdf's, then the saved map will be MUCH nicer.\n",
    "# The code should work fine, but for some reason my machine doesn't like it.  Maybe yours\n",
    "# will do better.  To try it out, try reading the pdf instead of the png.\n",
    "\n",
    "#im = plt.imread('./map.pdf')\n",
    "im = plt.imread('./map.png')\n",
    "\n",
    "# Display the map and save it as a pdf.\n",
    "m.imshow(im, interpolation='lanczos', origin='upper')\n",
    "plt.savefig('./dangerous_lead.pdf')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
